{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_토닥토닥봇_데이터전처리.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidot-kr/MLStudy/blob/master/BERT_%ED%86%A0%EB%8B%A5%ED%86%A0%EB%8B%A5%EB%B4%87_%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyLlLw0muJK4",
        "colab_type": "text"
      },
      "source": [
        "<h1>[BERT를 활용한 텍스트 분류]</h1>\n",
        "<h2>🥰토닥토닥봇 만들기 - 데이터 전처리 </h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGm20o5Y51Sm",
        "colab_type": "text"
      },
      "source": [
        "## #1. 실습 준비\n",
        "이번 모듈에서는 다운받은 엑셀 파일을 전처리해서 BERT 학습이 가능한 인풋으로 만드는 과정을 진행할 것입니다.\n",
        "\n",
        "전처리된 데이터셋을 저장할 수 있도록 구글 드라이브를 마운트하고, 필요한 라이브러리를 설치해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3GiiwyfmOqq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "199a83df-955e-4389-bdcf-b3f3a9965da6"
      },
      "source": [
        "## 구글 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLUI6zuUdTyz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b2a43bca-e9ab-46aa-fc81-68d780a8e3bb"
      },
      "source": [
        "## Tensorflow에서 BERT를 사용할 수 있는 라이브러리인 bert-for-tf2와 konlpy 패키지 설치\n",
        "## 실습에 필요한 라이브러리 로딩\n",
        "!pip install bert-for-tf2\n",
        "!pip install konlpy\n",
        "\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import bert\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/df/ab6d927d6162657f30eb0ae3c534c723c28c191a9caf6ee68ec935df3d0b/bert-for-tf2-0.14.5.tar.gz (40kB)\n",
            "\r\u001b[K     |████████                        | 10kB 19.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 30kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 1.6MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.5-cp36-none-any.whl size=30315 sha256=6a4b49f235a9cf47bb4665387a0cca12845fd2332b071519a3370ec84300c41b\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/70/a2/be357037dd2cbdcaeb0add1fdf083be6a600ca65ee1f68751c\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7302 sha256=afef27f7b49499ba76f072347171bbb6b35ff582f81d152213eb3c6beb5b4c21\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19473 sha256=d155ffdc007b6dab4485766acde69ab2ac88494aeb13363175287a5965ecf504\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.5 params-flow-0.8.2 py-params-0.9.7\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 56.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Collecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/f7/a368401e630f0e390dd0e62c39fb928e5b23741b53c2360ee7d376660927/JPype1-1.0.2-cp36-cp36m-manylinux2010_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 42.9MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: beautifulsoup4, tweepy, JPype1, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.0.2 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 tweepy-3.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3BK2fY7GkL5",
        "colab_type": "text"
      },
      "source": [
        "<font color = \"red\"><b>\n",
        "gdrive/NLP에</font>    \n",
        "\n",
        "- \"Wellness_data_train.json\"\n",
        "- \"Wellness_data_test.json\"\n",
        "- \"Wellness_response.json\"   \n",
        "\n",
        "데이터가 없는 경우 아래의 코드 복사해 실행</b>\n",
        "\n",
        "```\n",
        "\"\"\"MAKE DATASET\"\"\"\n",
        "import random\n",
        "import pandas as pd\n",
        "random.seed(1)\n",
        "\n",
        "def isNaN(num):\n",
        "    return num != num\n",
        "\n",
        "EXCEL_FILE_NALE = \"/content/웰니스_대화_스크립트_데이터셋.xlsx\"\n",
        "data = pd.read_excel(EXCEL_FILE_NALE)\n",
        "\n",
        "DATA = []\n",
        "RESPONSE = {}\n",
        "\n",
        "for i in range(len(data[\"구분\"])):\n",
        "  label = data[\"구분\"][i]\n",
        "  label_split = label.split(\"/\")\n",
        "  label_1 = \"/\".join(label_split[:2])\n",
        "  sent = data[\"유저\"][i]\n",
        "  if label_1 != \"모호함\":\n",
        "    DATA.append([\"Sent_{}\".format(i), sent, label_1, label])\n",
        "    if label_1 in RESPONSE:  \n",
        "      if not isNaN(data[\"챗봇\"][i]): \n",
        "        RESPONSE[label_1].append(data[\"챗봇\"][i])\n",
        "    else: \n",
        "      if not isNaN(data[\"챗봇\"][i]):  \n",
        "        RESPONSE[label_1] = [data[\"챗봇\"][i]]\n",
        "\n",
        "\"\"\"random shuffle & make them into train/test set\"\"\"\n",
        "labels = [dat[2] for dat in DATA]\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(DATA, random_state = 2020, stratify = labels, test_size = 400)\n",
        "\n",
        "with open(\"/content/gdrive/My Drive/NLP/Wellness_data_train.json\",\"w\") as f:\n",
        "  f.write(json.dumps(train))\n",
        "with open(\"/content/gdrive/My Drive/NLP/Wellness_data_test.json\",\"w\") as f:\n",
        "  f.write(json.dumps(test))\n",
        "with open(\"/content/gdrive/My Drive/NLP/Wellness_response.json\",\"w\") as f:\n",
        "  f.write(json.dumps(RESPONSE))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6TuaxlQwiob",
        "colab_type": "text"
      },
      "source": [
        "## #2. 사전학습된 BERT 모델 로드하기\n",
        "저희는 구글에서 공개한 다국어 모델인 BERT-multilingual-cased 모델을 활용해 의도분류 모델로 fine-tuning할 것입니다.\n",
        "\n",
        "Tensorflow HUB에서는 사전학습된 모델 과 토크나이저 등 학습에 필요한 것들를 제공하고 있습니다.\n",
        "\n",
        "아래 코드는 <b>hub.KerasLayer</b> 함수를 통해 다국어 모델을 다운로드하는 코드입니다.\n",
        "\n",
        "이때 fine-tuning이 가능하도록 레이어의 trainable 옵션을 <font color=\"blue\">True</font>로 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvcbF_WliIPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BERT_MODEL_HUB = 'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2'\n",
        "\n",
        "# BERT layer 가져오기\n",
        "bert_layer = hub.KerasLayer(BERT_MODEL_HUB, trainable=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hktrgEmu3pz",
        "colab_type": "text"
      },
      "source": [
        "## #3. BERT 토크나이저 로드하기\n",
        "BERT 모델을 fine-tuning하기 위해서는 사전학습된 BERT가 사용한 단어사전을 이용해 인풋을 처리해야 하겠지요?\n",
        "\n",
        "BERT 토크나이저를 로드하고, 한국어 형태소분석과 WordPiece Tokenizer을 적용해 토크나이즈하는 연습을 해보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSu5oGnxwqgd",
        "colab_type": "text"
      },
      "source": [
        "#### Step 1. 토크나이저 로딩하기\n",
        "- hub를 통해 로드한 bert_layer에는 사전학습에 활용한 단어사전 정보가 담겨있습니다.\n",
        "- bert-for-tf2 라이브러리에서 제공하는 bert_tokenization 함수를 이용해 BERT 토크나이저를 로드하겠습니다.\n",
        "- 아래 코드는 tokenizer라는 이름으로 BERT 토크나이저를 로딩하고, 단어사전 내의 토큰들을 확인하는 코드입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsSbSQ10uj7m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "5b53c949-d130-43fc-a922-cb290e428f1d"
      },
      "source": [
        "from  bert.tokenization import bert_tokenization\n",
        "\n",
        "# vocab_file 가져오기\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "# 소문자화를 하는지 여부 가져오기\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "\n",
        "print(\"vocab file:\", vocab_file) # vocab 사전을 저장한 경로\n",
        "print(\"do_lower_case:\", do_lower_case) # 소문자화 여부 (다국어 모델의 경우 False임)\n",
        "\n",
        "# 토크나이저 로딩\n",
        "tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "\n",
        "# vocab 사전 확인하기\n",
        "print(\"단어사전에 있는 토큰 개수:\", len(tokenizer.vocab))\n",
        "print(\"예시:\", list(tokenizer.vocab.keys())[:300])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab file: b'/tmp/tfhub_modules/3e9209b9f2a53dfa4e6d93250dfceb5e64d73b66/assets/vocab.txt'\n",
            "do_lower_case: False\n",
            "단어사전에 있는 토큰 개수: 119547\n",
            "예시: ['[PAD]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]', '[unused50]', '[unused51]', '[unused52]', '[unused53]', '[unused54]', '[unused55]', '[unused56]', '[unused57]', '[unused58]', '[unused59]', '[unused60]', '[unused61]', '[unused62]', '[unused63]', '[unused64]', '[unused65]', '[unused66]', '[unused67]', '[unused68]', '[unused69]', '[unused70]', '[unused71]', '[unused72]', '[unused73]', '[unused74]', '[unused75]', '[unused76]', '[unused77]', '[unused78]', '[unused79]', '[unused80]', '[unused81]', '[unused82]', '[unused83]', '[unused84]', '[unused85]', '[unused86]', '[unused87]', '[unused88]', '[unused89]', '[unused90]', '[unused91]', '[unused92]', '[unused93]', '[unused94]', '[unused95]', '[unused96]', '[unused97]', '[unused98]', '[unused99]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '<S>', '<T>', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '¡', '¢', '£', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '®', '°', '±', '²', '³', 'µ', '¶', '·', '¹', 'º', '»', '¼', '½', '¾', '¿', 'À', 'Á', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ð', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ø', 'Ú', 'Ü', 'Ý', 'Þ', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', '÷', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'þ', 'ÿ', 'Ā', 'ā', 'Ă', 'ă', 'Ą', 'ą', 'Ć', 'ć', 'Č', 'č', 'Ď', 'ď', 'Đ', 'đ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxT95BwawwXU",
        "colab_type": "text"
      },
      "source": [
        "#### Step 2. 형태소 분석 + BERT 토크나이즈 실행하기\n",
        "\n",
        "- 한국어의 경우, 형태소 분석을 진행한 문장에 BERT 토크나이즈를 적용하는 것이 좋은 것으로 알려져 있습니다.\n",
        "- konlpy의 Okt 형태소 분석기를 통해 형태소 분석을 진행하고, BERT 토크나이저를 적용하는 코드를 실행해보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i20ZPyEMQJYl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1b916eaf-0e17-4476-c4c6-796f7f5bc369"
      },
      "source": [
        "\"\"\" 형태소 분석 함수 \"\"\"\n",
        "from konlpy.tag import Okt\n",
        "okt=Okt()\n",
        "def tokenize(lines):\n",
        "  return okt.morphs(lines)\n",
        "\n",
        "sentence = \"버트로 토크나이즈하는 예제\"\n",
        "print(\"** 원본 문장 :\", sentence)\n",
        "\n",
        "# basic_tokenizer로 문장 쪼개기\n",
        "tokenized_sentence = tokenize(sentence)\n",
        "print(\"** 형태소 분석 결과 :\",tokenized_sentence)\n",
        "\n",
        "# BPE로 문장 쪼개기\n",
        "sub_tokens = tokenizer.tokenize(\" \".join(tokenized_sentence))\n",
        "print(\"** BERT 토크나이즈 결과 :\", sub_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** 원본 문장 : 버트로 토크나이즈하는 예제\n",
            "** 형태소 분석 결과 : ['버트', '로', '토크', '나', '이즈', '하', '는', '예제']\n",
            "** BERT 토크나이즈 결과 : ['버', '##트', '로', '토', '##크', '나', '이', '##즈', '하', '는', '예', '##제']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPOwbFPOwzIU",
        "colab_type": "text"
      },
      "source": [
        "#### Step 3. BERT 토크나이징된 토큰을 인풋 인덱스로 바꾸기\n",
        "\n",
        "- 이제 토큰화된 토큰들을 모델이 처리할 수 있는 정수 인덱스로 매핑하겠습니다.\n",
        "- 이 과정은 토크나이저의 convert_tokens_to_ids 매서드를 이용해 간단하게 진행할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGhw8GKGQJUe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8487ce70-431b-4f08-d943-b3fa31637720"
      },
      "source": [
        "# 모델 인풋 인덱스로 바꾸기\n",
        "input_ids = tokenizer.convert_tokens_to_ids(sub_tokens)\n",
        "print(\"** 매핑된 인덱스:\", input_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** 매핑된 인덱스: [9336, 15184, 9202, 9873, 20308, 8982, 9638, 24891, 9952, 9043, 9576, 17730]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hagwgDY9wIqQ",
        "colab_type": "text"
      },
      "source": [
        "## #4. 텍스트 분류 모델을 위한 BERT 인풋 생성하기 \n",
        "\n",
        "이제 BERT 모델 fine-tuning에 필요한 세 가지 tensor들을 만들어보겠습니다. \n",
        "\n",
        "- Input_ids : BERT 토크나이징을 적용한 토큰에 대한 정수 인덱스\n",
        "- Segment_ids : 각 토큰이 [문장 1]에 해당하는지 [문장 2]에 해당하는지 나타내는 인덱스\n",
        "- Input_mask : 실제로 토큰이 존재하는 부분은 1, [PAD]로 채운 부분은 0으로 표시한 인덱스 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sylgE4QkxjPZ",
        "colab_type": "text"
      },
      "source": [
        "#### step 1. 데이터 읽어오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAUK15gNmTXB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ad821f96-9689-4238-caf8-97119b066a37"
      },
      "source": [
        "import json\n",
        "filename = \"/content/gdrive/My Drive/NLP/Wellness_data_train.json\"\n",
        "\n",
        "with open(filename) as f:\n",
        "  train = json.loads(f.read())\n",
        "print(\"# of Train =\", len(train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of Train = 4651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6bQ-1pWTXwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "4903993e-a0c2-4331-94d4-f979581c9b19"
      },
      "source": [
        "## 데이터셋 확인\n",
        "print(train[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Sent_4393', '뭔가 하루종일 이렇게 들뜬 기분이다 보니까 잠도 잘 안 와.', '증상/불면', '증상/불면'], ['Sent_603', '아무한테나 화내고 그러지는 않아.', '감정/분노', '감정/분노'], ['Sent_4224', '잠자리에 누워도 맨날 뒤척이고... 잠을 제대로 잘 수 있을 리가 없지.', '증상/불면', '증상/불면'], ['Sent_3849', '5일 전에는 새벽에 일어나서 화장실을 가다가 순간적으로 정신을 잃었어.', '증상/기절', '증상/기절'], ['Sent_666', '그냥 감정이입이 심하게 되고 불안감도 잘 느끼는 것 같아요.', '감정/불안감', '감정/불안감']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hALX4zcnxnfy",
        "colab_type": "text"
      },
      "source": [
        "#### Step 2. BERT 인풋 만들기 - 함수 정의\n",
        "\n",
        "- 학습을 위해 자연어 문장에 BPE를 적용하고, input_ids, segment_ids, input_masks를 생성하는 함수를 정의하겠습니다.\n",
        "- 이 함수에서는 학습을 위해 Ground Truth 라벨까지 정보를 InputFeatures라는 클래스를 이용해 저장하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLNHWwT91GgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5HbPHS3SS8-",
        "colab_type": "text"
      },
      "source": [
        "<font color = \"red\">[MISSION] convert_examples_to_features 함수 완성하기</font>   \n",
        "\n",
        "convert_examples_to_features는 \n",
        "- examples (데이터의 리스트) \n",
        "- max_seq_length (인풋 시퀀스의 최대 시퀀스 길이)\n",
        "- tokenizer (로딩한 BERT토크나이저)\n",
        "- label_map (인덱스 매핑 사전)   \n",
        "\n",
        "를 인풋으로 받아 InputFeatures를 반환하는 함수입니다.   \n",
        "아래 코드에서 [★CODE] 부분을 수정하여 함수가 정상적으로 작동하도록 수정해보세요.\n",
        "\n",
        "<i>** 정답은 Wire 페이지를 통해 확인할 수 있습니다. </i>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRBcsKMWdeoF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_examples_to_features(examples, max_seq_length, tokenizer, label_map):\n",
        "    # features : 리턴할 InputFeatures를 담을 리스트\n",
        "    features = []\n",
        "\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        \"\"\"\n",
        "        함수의 인풋으로 들어오는 리스트는 [문장번호 - 문장 - 라벨]의 순서로 이루어져 있습니다.\n",
        "        문장을 text 변수에 저장하고, 라벨을 label 변수에 저장하였습니다.\n",
        "        label이 어떤 정수 인덱스에 해당하는지는 label_map을 통해 정보를 받아 label_id 변수에 저장합니다.\n",
        "        \"\"\"\n",
        "        text = example[1] # 문장\n",
        "        label = example[2] # 라벨\n",
        "        label_id = label_map[label]\n",
        "\n",
        "        ## STEP 1. 형태소 분석 + BPE를 통해 토큰화하기\n",
        "        otk_tokenized_text = [★CODE 1★] # text에 한국어 형태소 분석 적용\n",
        "        tokens_a = [★CODE 2★] # otk_tokenized_text에 BERT 토크나이즈 적용\n",
        "\n",
        "        ## 주어진 최대 시퀀스 길이보다 긴 문장은 끝부분을 잘라냄\n",
        "        ## [CLS], [SEP] 토큰이 반드시 포함되므로 max_seq_length - 2보다 긴 부분 자르기\n",
        "        if len(tokens_a) > max_seq_length - 2:\n",
        "            tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "        ## STEP 2. BERT 인풋 형식 맞추기 : [CLS] 문장 [SEP] 형태 만들기\n",
        "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
        "\n",
        "        ## STEP 3. segment_ids 만들기 \n",
        "        ## -> 단일 문장이므로 모두 '0' 인덱스를 부여함\n",
        "        segment_ids = [★CODE 3★]\n",
        "\n",
        "        ## STEP 4. 토큰을 input_ids로 변환\n",
        "        ## hint: tokenizer.convert_tokens_to_ids 사용\n",
        "        input_ids = [★CODE 4★]\n",
        "\n",
        "        ## STEP 5. 인풋 마스크 만들기 \n",
        "        ## -> 실제 토큰이 있는 부분을 1로 채움\n",
        "        input_mask = [★CODE 5★]\n",
        "\n",
        "        ## max_seq_length보다 모자른 부분을 padding으로 채움 \n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        if ex_index < 3:\n",
        "            print(\"*** Example ***\")\n",
        "            print(\"tokens: %s\" % \" \".join(\n",
        "                    [str(x) for x in tokens]))\n",
        "\n",
        "            print(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            print(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "            print(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "            print(\"label: %d\" % (label_id))\n",
        "\n",
        "        features.append(\n",
        "                InputFeatures(input_ids=input_ids,\n",
        "                              input_mask=input_mask,\n",
        "                              segment_ids=segment_ids,\n",
        "                              label_id=label_id))\n",
        "    return features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhFAFdwxacX6",
        "colab_type": "text"
      },
      "source": [
        "#### Step 3. BERT 인풋 만들기 - 데이터에 함수 적용\n",
        "- 이제 정의한 convert_examples_to_features 함수를 이용해 우리의 train 데이터를 BERT인풋으로 만듭니다.\n",
        "- 먼저 176개의 라벨을 0~175의 정수 인덱스로 매핑하는 label_map을 생성합니다.\n",
        "- 이후 train 데이터에 함수를 적용해 train_features라는 변수에 저장하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NBEQ4iBbkVv",
        "colab_type": "text"
      },
      "source": [
        "<font color = \"red\">[MISSION] 전처리 결과 input_ids, input_mask, segment_id가 잘 생성되었는지 확인하세요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7FZaDBn9EoC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "47bf1b9a-c71b-4311-b07d-f3e926103162"
      },
      "source": [
        "# label mapping 사전 만들기\n",
        "label_map = {}\n",
        "label_id = 0\n",
        "for dat in train:\n",
        "  if dat[2] not in label_map:\n",
        "    label_map[dat[2]] = label_id\n",
        "    label_id += 1\n",
        "\n",
        "## 최대 문장 길이를 128으로 설정하고 feature 만들기\n",
        "\n",
        "MAX_SEQ_LENGTH = 128\n",
        "print(\"*** Convert Train Examples ***\")\n",
        "train_features = convert_examples_to_features(train, MAX_SEQ_LENGTH, tokenizer, label_map)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** Convert Train Examples ***\n",
            "*** Example ***\n",
            "tokens: [CLS] 뭔 ##가 하 ##루 종 ##일 이 ##렇게 들 ##뜬 기 ##분 이다 보 니 ##까 잠 ##도 잘 안 와 . [SEP]\n",
            "input_ids: 101 9304 11287 9952 35866 9684 18392 9638 82838 9117 118847 8932 37712 30919 9356 9049 118671 9655 12092 9654 9521 9590 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label: 0\n",
            "*** Example ***\n",
            "tokens: [CLS] 아 ##무 한 테 나 화 내 ##고 그 ##러 ##지는 않 ##아 . [SEP]\n",
            "input_ids: 101 9519 32537 9954 9866 8982 9993 8996 11664 8924 30873 32815 9523 16985 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label: 1\n",
            "*** Example ***\n",
            "tokens: [CLS] 잠 ##자 ##리 에 누 ##워 ##도 맨 ##날 뒤 ##척 이 ##고 . . . 잠 을 제 ##대로 잘 수 있을 리 ##가 없 ##지 . [SEP]\n",
            "input_ids: 101 9655 13764 12692 9559 9032 69592 12092 9260 41919 9109 119259 9638 11664 119 119 119 9655 9633 9672 37601 9654 9460 68943 9238 11287 9555 12508 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhDvTHhodlmx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "2cb8ee21-cd89-4a11-b988-b7f2bf118542"
      },
      "source": [
        "print(label_map)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'증상/불면': 0, '감정/분노': 1, '증상/기절': 2, '감정/불안감': 3, '증상/두근거림': 4, '배경/건강문제': 5, '배경/취업': 6, '감정/우울감': 7, '배경/대인관계': 8, '배경/어린시절': 9, '감정/자살충동': 10, '감정/외로움': 11, '증상/자살시도': 12, '감정/걱정': 13, '배경/학업': 14, '증상/무기력': 15, '증상/식욕저하': 16, '일반대화': 17, '배경/결혼': 18, '상태/증상지속': 19, '배경/학교': 20, '감정/짜증': 21, '배경/부모': 22, '증상/두통': 23, '증상/피해망상': 24, '감정/자존감저하': 25, '배경/성격': 26, '증상/기억상실': 27, '배경/사업': 28, '감정/감정조절이상': 29, '배경/전연인': 30, '감정/불편감': 31, '배경/경제적문제': 32, '치료이력/병원내원': 33, '배경/친구': 34, '배경/가족': 35, '배경/여자친구': 36, '치료이력/검사': 37, '증상/어지러움': 38, '감정/부정적사고': 39, '감정/눈물': 40, '배경/시댁': 41, '증상/반복행동': 42, '감정/좌절': 43, '배경/자녀': 44, '감정/무력감': 45, '배경/사고': 46, '배경/직장': 47, '증상/기억력저하': 48, '증상/은둔': 49, '증상/환청': 50, '증상/이명': 51, '부가설명': 52, '감정/심란': 53, '감정/답답': 54, '감정/힘듦': 55, '감정/공허감': 56, '배경/남편': 57, '증상/반복사고': 58, '감정/무서움': 59, '감정/두려움': 60, '증상/피로': 61, '감정/속상함': 62, '감정/통제력상실': 63, '배경/생활': 64, '배경/종교': 65, '감정/자괴감': 66, '증상/환각': 67, '배경/음주': 68, '배경/대학': 69, '자가치료/심리조절': 70, '감정/당황': 71, '감정/즐거움': 72, '배경/남자친구': 73, '증상/생리불순': 74, '감정/불쾌감': 75, '증상/공황발작': 76, '감정/억울함': 77, '감정/배신감': 78, '증상/호흡곤란': 79, '감정/신경쓰임': 80, '배경/문제': 81, '배경/귀국': 82, '배경/애완동물': 83, '배경/타인': 84, '감정/절망감': 85, '증상/이인감': 86, '증상/대화기피': 87, '감정/서운함': 88, '내원이유/치료': 89, '감정/괴로움': 90, '치료이력/응급실': 91, '감정/자신감저하': 92, '감정/화': 93, '감정/충격': 94, '증상/집중력저하': 95, '증상/자해': 96, '배경/이사': 97, '배경/연애': 98, '감정/살인욕구': 99, '감정/불신': 100, '감정/기분저하': 101, '감정/의욕상실': 102, '배경/군대': 103, '감정/공포': 104, '증상/과대망상': 105, '배경/자각': 106, '상태/양호': 107, '증상/폭식': 108, '감정/예민함': 109, '감정/모호함': 110, '증상/과수면': 111, '증상/대인기피': 112, '증상/힘빠짐': 113, '배경/임신': 114, '증상/체중감소': 115, '배경/이혼': 116, '감정/긴장': 117, '증상/죽음공포': 118, '감정/의기소침': 119, '현재상태/증상지속': 120, '증상/체력저하': 121, '증상/통증': 122, '증상/악몽': 123, '증상/공격적성향': 124, '증상/컨디션저조': 125, '증상/성욕상승': 126, '감정/고독감': 127, '증상/가슴통증': 128, '배경/육아': 129, '감정/후회': 130, '배경/아르바이트': 131, '내원이유/상담': 132, '배경/공부': 133, '현재상태/증상악화': 134, '증상/시력저하': 135, '증상/신체이상': 136, '증상/건강염려': 137, '감정/무미건조': 138, '감정/불만': 139, '감정/미움': 140, '증상/알코올의존': 141, '증상/기절예기': 142, '감정/슬픔': 143, '감정/생각': 144, '증상/만성피로': 145, '현재상태/증상감소': 146, '감정/과민반응': 147, '증상/가슴떨림': 148, '감정/기시감': 149, '감정/미안함': 150, '감정/허무함': 151, '증상/체중증가': 152, '감정/멍함': 153, '감정/죄책감': 154, '증상/메스꺼움': 155, '감정/비관적': 156, '자가치료/운동': 157, '자가치료/충분한휴식': 158, '증상/발작': 159, '증상/인지기능저하': 160, '내원이유/의사소견': 161, '증상/저림현상': 162, '증상/가슴답답': 163, '증상/성격변화': 164, '증상/소화불량': 165, '증상/편두통': 166, '증상/떨림': 167, '배경/유학': 168, '상태/증상감소': 169, '감정/창피함': 170, '증상/속쓰림': 171, '감정/곤혹감': 172, '원인/없음': 173, '감정/초조함': 174, '배경/진로': 175}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONz8hu3vb7II",
        "colab_type": "text"
      },
      "source": [
        "#### Step 4. 전처리 결과 확인 & 드라이브에 저장\n",
        "- BERT 토크나이징 결과, [UNK]가 얼마나 생성되었는지 확인해보겠습니다.\n",
        "- 현재 데이터의 경우, 학습 데이터 중 1.34%가 [UNK] 토큰으로 떨어진 것을 확인할 수 있습니다.\n",
        "- 구글의 모델은 102국어에 대해 학습된 모델으로, 한국어만으로 학습한 모델에 비해 UNK 비율이 높은 편입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEEvpbzz8R2W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4cce563e-c57b-4070-a163-798a54982929"
      },
      "source": [
        "tot_tokens = 0\n",
        "num_oov = 0\n",
        "for feat in train_features:\n",
        "  num_oov += sum([s == 100 for s in feat.input_ids])\n",
        "  tot_tokens += sum([s != 0 for s in feat.input_ids])\n",
        "\n",
        "print(\"학습 데이터 중 {}개 ({:.2f}%) 토큰은 [UNK] 처리됨\".format(num_oov, 100*num_oov/tot_tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습 데이터 중 1351개 (1.34%) 토큰은 [UNK] 처리됨\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzsJXe4ncnqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open(\"/content/gdrive/My Drive/NLP/BERT_CLS_train_features.pkl\", \"wb\") as f:\n",
        "  pickle.dump(train_features, f)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}