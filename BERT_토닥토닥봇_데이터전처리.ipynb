{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_í† ë‹¥í† ë‹¥ë´‡_ë°ì´í„°ì „ì²˜ë¦¬.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidot-kr/MLStudy/blob/master/BERT_%ED%86%A0%EB%8B%A5%ED%86%A0%EB%8B%A5%EB%B4%87_%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyLlLw0muJK4",
        "colab_type": "text"
      },
      "source": [
        "<h1>[BERTë¥¼ í™œìš©í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜]</h1>\n",
        "<h2>ğŸ¥°í† ë‹¥í† ë‹¥ë´‡ ë§Œë“¤ê¸° - ë°ì´í„° ì „ì²˜ë¦¬ </h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGm20o5Y51Sm",
        "colab_type": "text"
      },
      "source": [
        "## #1. ì‹¤ìŠµ ì¤€ë¹„\n",
        "ì´ë²ˆ ëª¨ë“ˆì—ì„œëŠ” ë‹¤ìš´ë°›ì€ ì—‘ì…€ íŒŒì¼ì„ ì „ì²˜ë¦¬í•´ì„œ BERT í•™ìŠµì´ ê°€ëŠ¥í•œ ì¸í’‹ìœ¼ë¡œ ë§Œë“œëŠ” ê³¼ì •ì„ ì§„í–‰í•  ê²ƒì…ë‹ˆë‹¤.\n",
        "\n",
        "ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ì„ ì €ì¥í•  ìˆ˜ ìˆë„ë¡ êµ¬ê¸€ ë“œë¼ì´ë¸Œë¥¼ ë§ˆìš´íŠ¸í•˜ê³ , í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3GiiwyfmOqq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "199a83df-955e-4389-bdcf-b3f3a9965da6"
      },
      "source": [
        "## êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLUI6zuUdTyz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b2a43bca-e9ab-46aa-fc81-68d780a8e3bb"
      },
      "source": [
        "## Tensorflowì—ì„œ BERTë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ bert-for-tf2ì™€ konlpy íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "## ì‹¤ìŠµì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”©\n",
        "!pip install bert-for-tf2\n",
        "!pip install konlpy\n",
        "\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import bert\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/df/ab6d927d6162657f30eb0ae3c534c723c28c191a9caf6ee68ec935df3d0b/bert-for-tf2-0.14.5.tar.gz (40kB)\n",
            "\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 10kB 19.7MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 30kB 2.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40kB 1.6MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.5-cp36-none-any.whl size=30315 sha256=6a4b49f235a9cf47bb4665387a0cca12845fd2332b071519a3370ec84300c41b\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/70/a2/be357037dd2cbdcaeb0add1fdf083be6a600ca65ee1f68751c\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7302 sha256=afef27f7b49499ba76f072347171bbb6b35ff582f81d152213eb3c6beb5b4c21\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19473 sha256=d155ffdc007b6dab4485766acde69ab2ac88494aeb13363175287a5965ecf504\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.5 params-flow-0.8.2 py-params-0.9.7\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19.4MB 56.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Collecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/f7/a368401e630f0e390dd0e62c39fb928e5b23741b53c2360ee7d376660927/JPype1-1.0.2-cp36-cp36m-manylinux2010_x86_64.whl (3.8MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8MB 42.9MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: beautifulsoup4, tweepy, JPype1, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.0.2 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 tweepy-3.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3BK2fY7GkL5",
        "colab_type": "text"
      },
      "source": [
        "<font color = \"red\"><b>\n",
        "gdrive/NLPì—</font>    \n",
        "\n",
        "- \"Wellness_data_train.json\"\n",
        "- \"Wellness_data_test.json\"\n",
        "- \"Wellness_response.json\"   \n",
        "\n",
        "ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì•„ë˜ì˜ ì½”ë“œ ë³µì‚¬í•´ ì‹¤í–‰</b>\n",
        "\n",
        "```\n",
        "\"\"\"MAKE DATASET\"\"\"\n",
        "import random\n",
        "import pandas as pd\n",
        "random.seed(1)\n",
        "\n",
        "def isNaN(num):\n",
        "    return num != num\n",
        "\n",
        "EXCEL_FILE_NALE = \"/content/ì›°ë‹ˆìŠ¤_ëŒ€í™”_ìŠ¤í¬ë¦½íŠ¸_ë°ì´í„°ì…‹.xlsx\"\n",
        "data = pd.read_excel(EXCEL_FILE_NALE)\n",
        "\n",
        "DATA = []\n",
        "RESPONSE = {}\n",
        "\n",
        "for i in range(len(data[\"êµ¬ë¶„\"])):\n",
        "  label = data[\"êµ¬ë¶„\"][i]\n",
        "  label_split = label.split(\"/\")\n",
        "  label_1 = \"/\".join(label_split[:2])\n",
        "  sent = data[\"ìœ ì €\"][i]\n",
        "  if label_1 != \"ëª¨í˜¸í•¨\":\n",
        "    DATA.append([\"Sent_{}\".format(i), sent, label_1, label])\n",
        "    if label_1 in RESPONSE:  \n",
        "      if not isNaN(data[\"ì±—ë´‡\"][i]): \n",
        "        RESPONSE[label_1].append(data[\"ì±—ë´‡\"][i])\n",
        "    else: \n",
        "      if not isNaN(data[\"ì±—ë´‡\"][i]):  \n",
        "        RESPONSE[label_1] = [data[\"ì±—ë´‡\"][i]]\n",
        "\n",
        "\"\"\"random shuffle & make them into train/test set\"\"\"\n",
        "labels = [dat[2] for dat in DATA]\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(DATA, random_state = 2020, stratify = labels, test_size = 400)\n",
        "\n",
        "with open(\"/content/gdrive/My Drive/NLP/Wellness_data_train.json\",\"w\") as f:\n",
        "  f.write(json.dumps(train))\n",
        "with open(\"/content/gdrive/My Drive/NLP/Wellness_data_test.json\",\"w\") as f:\n",
        "  f.write(json.dumps(test))\n",
        "with open(\"/content/gdrive/My Drive/NLP/Wellness_response.json\",\"w\") as f:\n",
        "  f.write(json.dumps(RESPONSE))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6TuaxlQwiob",
        "colab_type": "text"
      },
      "source": [
        "## #2. ì‚¬ì „í•™ìŠµëœ BERT ëª¨ë¸ ë¡œë“œí•˜ê¸°\n",
        "ì €í¬ëŠ” êµ¬ê¸€ì—ì„œ ê³µê°œí•œ ë‹¤êµ­ì–´ ëª¨ë¸ì¸ BERT-multilingual-cased ëª¨ë¸ì„ í™œìš©í•´ ì˜ë„ë¶„ë¥˜ ëª¨ë¸ë¡œ fine-tuningí•  ê²ƒì…ë‹ˆë‹¤.\n",
        "\n",
        "Tensorflow HUBì—ì„œëŠ” ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ ê³¼ í† í¬ë‚˜ì´ì € ë“± í•™ìŠµì— í•„ìš”í•œ ê²ƒë“¤ë¥¼ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì•„ë˜ ì½”ë“œëŠ”Â <b>hub.KerasLayer</b> í•¨ìˆ˜ë¥¼ í†µí•´ ë‹¤êµ­ì–´ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.\n",
        "\n",
        "ì´ë•Œ fine-tuningì´ ê°€ëŠ¥í•˜ë„ë¡ ë ˆì´ì–´ì˜ trainable ì˜µì…˜ì„ <font color=\"blue\">True</font>ë¡œ ì„¤ì •í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvcbF_WliIPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BERT_MODEL_HUB = 'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2'\n",
        "\n",
        "# BERT layer ê°€ì ¸ì˜¤ê¸°\n",
        "bert_layer = hub.KerasLayer(BERT_MODEL_HUB, trainable=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hktrgEmu3pz",
        "colab_type": "text"
      },
      "source": [
        "## #3. BERT í† í¬ë‚˜ì´ì € ë¡œë“œí•˜ê¸°\n",
        "BERT ëª¨ë¸ì„ fine-tuningí•˜ê¸° ìœ„í•´ì„œëŠ” ì‚¬ì „í•™ìŠµëœ BERTê°€ ì‚¬ìš©í•œ ë‹¨ì–´ì‚¬ì „ì„ ì´ìš©í•´ ì¸í’‹ì„ ì²˜ë¦¬í•´ì•¼ í•˜ê² ì§€ìš”?\n",
        "\n",
        "BERT í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ê³ , í•œêµ­ì–´ í˜•íƒœì†Œë¶„ì„ê³¼ WordPiece Tokenizerì„ ì ìš©í•´ í† í¬ë‚˜ì´ì¦ˆí•˜ëŠ” ì—°ìŠµì„ í•´ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSu5oGnxwqgd",
        "colab_type": "text"
      },
      "source": [
        "#### Step 1. í† í¬ë‚˜ì´ì € ë¡œë”©í•˜ê¸°\n",
        "- hubë¥¼ í†µí•´ ë¡œë“œí•œ bert_layerì—ëŠ” ì‚¬ì „í•™ìŠµì— í™œìš©í•œ ë‹¨ì–´ì‚¬ì „ ì •ë³´ê°€ ë‹´ê²¨ìˆìŠµë‹ˆë‹¤.\n",
        "- bert-for-tf2 ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” bert_tokenization í•¨ìˆ˜ë¥¼ ì´ìš©í•´ BERT í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ê² ìŠµë‹ˆë‹¤.\n",
        "- ì•„ë˜ ì½”ë“œëŠ” tokenizerë¼ëŠ” ì´ë¦„ìœ¼ë¡œ BERT í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë”©í•˜ê³ , ë‹¨ì–´ì‚¬ì „ ë‚´ì˜ í† í°ë“¤ì„ í™•ì¸í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsSbSQ10uj7m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "5b53c949-d130-43fc-a922-cb290e428f1d"
      },
      "source": [
        "from  bert.tokenization import bert_tokenization\n",
        "\n",
        "# vocab_file ê°€ì ¸ì˜¤ê¸°\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "# ì†Œë¬¸ìí™”ë¥¼ í•˜ëŠ”ì§€ ì—¬ë¶€ ê°€ì ¸ì˜¤ê¸°\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "\n",
        "print(\"vocab file:\", vocab_file) # vocab ì‚¬ì „ì„ ì €ì¥í•œ ê²½ë¡œ\n",
        "print(\"do_lower_case:\", do_lower_case) # ì†Œë¬¸ìí™” ì—¬ë¶€ (ë‹¤êµ­ì–´ ëª¨ë¸ì˜ ê²½ìš° Falseì„)\n",
        "\n",
        "# í† í¬ë‚˜ì´ì € ë¡œë”©\n",
        "tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "\n",
        "# vocab ì‚¬ì „ í™•ì¸í•˜ê¸°\n",
        "print(\"ë‹¨ì–´ì‚¬ì „ì— ìˆëŠ” í† í° ê°œìˆ˜:\", len(tokenizer.vocab))\n",
        "print(\"ì˜ˆì‹œ:\", list(tokenizer.vocab.keys())[:300])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab file: b'/tmp/tfhub_modules/3e9209b9f2a53dfa4e6d93250dfceb5e64d73b66/assets/vocab.txt'\n",
            "do_lower_case: False\n",
            "ë‹¨ì–´ì‚¬ì „ì— ìˆëŠ” í† í° ê°œìˆ˜: 119547\n",
            "ì˜ˆì‹œ: ['[PAD]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]', '[unused50]', '[unused51]', '[unused52]', '[unused53]', '[unused54]', '[unused55]', '[unused56]', '[unused57]', '[unused58]', '[unused59]', '[unused60]', '[unused61]', '[unused62]', '[unused63]', '[unused64]', '[unused65]', '[unused66]', '[unused67]', '[unused68]', '[unused69]', '[unused70]', '[unused71]', '[unused72]', '[unused73]', '[unused74]', '[unused75]', '[unused76]', '[unused77]', '[unused78]', '[unused79]', '[unused80]', '[unused81]', '[unused82]', '[unused83]', '[unused84]', '[unused85]', '[unused86]', '[unused87]', '[unused88]', '[unused89]', '[unused90]', '[unused91]', '[unused92]', '[unused93]', '[unused94]', '[unused95]', '[unused96]', '[unused97]', '[unused98]', '[unused99]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '<S>', '<T>', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', 'Â¡', 'Â¢', 'Â£', 'Â¥', 'Â¦', 'Â§', 'Â¨', 'Â©', 'Âª', 'Â«', 'Â¬', 'Â®', 'Â°', 'Â±', 'Â²', 'Â³', 'Âµ', 'Â¶', 'Â·', 'Â¹', 'Âº', 'Â»', 'Â¼', 'Â½', 'Â¾', 'Â¿', 'Ã€', 'Ã', 'Ã‚', 'Ãƒ', 'Ã„', 'Ã…', 'Ã†', 'Ã‡', 'Ãˆ', 'Ã‰', 'ÃŠ', 'Ã‹', 'ÃŒ', 'Ã', 'Ã', 'Ã', 'Ã‘', 'Ã’', 'Ã“', 'Ã”', 'Ã•', 'Ã–', 'Ã—', 'Ã˜', 'Ãš', 'Ãœ', 'Ã', 'Ã', 'ÃŸ', 'Ã ', 'Ã¡', 'Ã¢', 'Ã£', 'Ã¤', 'Ã¥', 'Ã¦', 'Ã§', 'Ã¨', 'Ã©', 'Ãª', 'Ã«', 'Ã¬', 'Ã­', 'Ã®', 'Ã¯', 'Ã°', 'Ã±', 'Ã²', 'Ã³', 'Ã´', 'Ãµ', 'Ã¶', 'Ã·', 'Ã¸', 'Ã¹', 'Ãº', 'Ã»', 'Ã¼', 'Ã½', 'Ã¾', 'Ã¿', 'Ä€', 'Ä', 'Ä‚', 'Äƒ', 'Ä„', 'Ä…', 'Ä†', 'Ä‡', 'ÄŒ', 'Ä', 'Ä', 'Ä', 'Ä', 'Ä‘']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxT95BwawwXU",
        "colab_type": "text"
      },
      "source": [
        "#### Step 2. í˜•íƒœì†Œ ë¶„ì„ + BERT í† í¬ë‚˜ì´ì¦ˆ ì‹¤í–‰í•˜ê¸°\n",
        "\n",
        "- í•œêµ­ì–´ì˜ ê²½ìš°, í˜•íƒœì†Œ ë¶„ì„ì„ ì§„í–‰í•œ ë¬¸ì¥ì— BERT í† í¬ë‚˜ì´ì¦ˆë¥¼ ì ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ì€ ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
        "- konlpyì˜ Okt í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ í†µí•´ í˜•íƒœì†Œ ë¶„ì„ì„ ì§„í–‰í•˜ê³ , BERT í† í¬ë‚˜ì´ì €ë¥¼ ì ìš©í•˜ëŠ” ì½”ë“œë¥¼ ì‹¤í–‰í•´ë³´ì„¸ìš”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i20ZPyEMQJYl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1b916eaf-0e17-4476-c4c6-796f7f5bc369"
      },
      "source": [
        "\"\"\" í˜•íƒœì†Œ ë¶„ì„ í•¨ìˆ˜ \"\"\"\n",
        "from konlpy.tag import Okt\n",
        "okt=Okt()\n",
        "def tokenize(lines):\n",
        "  return okt.morphs(lines)\n",
        "\n",
        "sentence = \"ë²„íŠ¸ë¡œ í† í¬ë‚˜ì´ì¦ˆí•˜ëŠ” ì˜ˆì œ\"\n",
        "print(\"** ì›ë³¸ ë¬¸ì¥ :\", sentence)\n",
        "\n",
        "# basic_tokenizerë¡œ ë¬¸ì¥ ìª¼ê°œê¸°\n",
        "tokenized_sentence = tokenize(sentence)\n",
        "print(\"** í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ :\",tokenized_sentence)\n",
        "\n",
        "# BPEë¡œ ë¬¸ì¥ ìª¼ê°œê¸°\n",
        "sub_tokens = tokenizer.tokenize(\" \".join(tokenized_sentence))\n",
        "print(\"** BERT í† í¬ë‚˜ì´ì¦ˆ ê²°ê³¼ :\", sub_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** ì›ë³¸ ë¬¸ì¥ : ë²„íŠ¸ë¡œ í† í¬ë‚˜ì´ì¦ˆí•˜ëŠ” ì˜ˆì œ\n",
            "** í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ : ['ë²„íŠ¸', 'ë¡œ', 'í† í¬', 'ë‚˜', 'ì´ì¦ˆ', 'í•˜', 'ëŠ”', 'ì˜ˆì œ']\n",
            "** BERT í† í¬ë‚˜ì´ì¦ˆ ê²°ê³¼ : ['ë²„', '##íŠ¸', 'ë¡œ', 'í† ', '##í¬', 'ë‚˜', 'ì´', '##ì¦ˆ', 'í•˜', 'ëŠ”', 'ì˜ˆ', '##ì œ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPOwbFPOwzIU",
        "colab_type": "text"
      },
      "source": [
        "#### Step 3. BERT í† í¬ë‚˜ì´ì§•ëœ í† í°ì„ ì¸í’‹ ì¸ë±ìŠ¤ë¡œ ë°”ê¾¸ê¸°\n",
        "\n",
        "- ì´ì œ í† í°í™”ëœ í† í°ë“¤ì„ ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë§¤í•‘í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "- ì´ ê³¼ì •ì€ í† í¬ë‚˜ì´ì €ì˜ convert_tokens_to_ids ë§¤ì„œë“œë¥¼ ì´ìš©í•´ ê°„ë‹¨í•˜ê²Œ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGhw8GKGQJUe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8487ce70-431b-4f08-d943-b3fa31637720"
      },
      "source": [
        "# ëª¨ë¸ ì¸í’‹ ì¸ë±ìŠ¤ë¡œ ë°”ê¾¸ê¸°\n",
        "input_ids = tokenizer.convert_tokens_to_ids(sub_tokens)\n",
        "print(\"** ë§¤í•‘ëœ ì¸ë±ìŠ¤:\", input_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** ë§¤í•‘ëœ ì¸ë±ìŠ¤: [9336, 15184, 9202, 9873, 20308, 8982, 9638, 24891, 9952, 9043, 9576, 17730]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hagwgDY9wIqQ",
        "colab_type": "text"
      },
      "source": [
        "## #4. í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ì„ ìœ„í•œ BERT ì¸í’‹ ìƒì„±í•˜ê¸°Â \n",
        "\n",
        "ì´ì œ BERT ëª¨ë¸ fine-tuningì— í•„ìš”í•œ ì„¸ ê°€ì§€ tensorë“¤ì„ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.Â \n",
        "\n",
        "- Input_ids : BERT í† í¬ë‚˜ì´ì§•ì„ ì ìš©í•œ í† í°ì— ëŒ€í•œ ì •ìˆ˜ ì¸ë±ìŠ¤\n",
        "- Segment_ids : ê° í† í°ì´ [ë¬¸ì¥ 1]ì— í•´ë‹¹í•˜ëŠ”ì§€ [ë¬¸ì¥ 2]ì— í•´ë‹¹í•˜ëŠ”ì§€ ë‚˜íƒ€ë‚´ëŠ” ì¸ë±ìŠ¤\n",
        "- Input_mask : ì‹¤ì œë¡œ í† í°ì´ ì¡´ì¬í•˜ëŠ” ë¶€ë¶„ì€ 1, [PAD]ë¡œ ì±„ìš´ ë¶€ë¶„ì€ 0ìœ¼ë¡œ í‘œì‹œí•œ ì¸ë±ìŠ¤ \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sylgE4QkxjPZ",
        "colab_type": "text"
      },
      "source": [
        "#### step 1. ë°ì´í„° ì½ì–´ì˜¤ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAUK15gNmTXB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ad821f96-9689-4238-caf8-97119b066a37"
      },
      "source": [
        "import json\n",
        "filename = \"/content/gdrive/My Drive/NLP/Wellness_data_train.json\"\n",
        "\n",
        "with open(filename) as f:\n",
        "  train = json.loads(f.read())\n",
        "print(\"# of Train =\", len(train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of Train = 4651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6bQ-1pWTXwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "4903993e-a0c2-4331-94d4-f979581c9b19"
      },
      "source": [
        "## ë°ì´í„°ì…‹ í™•ì¸\n",
        "print(train[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Sent_4393', 'ë­”ê°€ í•˜ë£¨ì¢…ì¼ ì´ë ‡ê²Œ ë“¤ëœ¬ ê¸°ë¶„ì´ë‹¤ ë³´ë‹ˆê¹Œ ì ë„ ì˜ ì•ˆ ì™€.', 'ì¦ìƒ/ë¶ˆë©´', 'ì¦ìƒ/ë¶ˆë©´'], ['Sent_603', 'ì•„ë¬´í•œí…Œë‚˜ í™”ë‚´ê³  ê·¸ëŸ¬ì§€ëŠ” ì•Šì•„.', 'ê°ì •/ë¶„ë…¸', 'ê°ì •/ë¶„ë…¸'], ['Sent_4224', 'ì ìë¦¬ì— ëˆ„ì›Œë„ ë§¨ë‚  ë’¤ì²™ì´ê³ ... ì ì„ ì œëŒ€ë¡œ ì˜ ìˆ˜ ìˆì„ ë¦¬ê°€ ì—†ì§€.', 'ì¦ìƒ/ë¶ˆë©´', 'ì¦ìƒ/ë¶ˆë©´'], ['Sent_3849', '5ì¼ ì „ì—ëŠ” ìƒˆë²½ì— ì¼ì–´ë‚˜ì„œ í™”ì¥ì‹¤ì„ ê°€ë‹¤ê°€ ìˆœê°„ì ìœ¼ë¡œ ì •ì‹ ì„ ìƒì—ˆì–´.', 'ì¦ìƒ/ê¸°ì ˆ', 'ì¦ìƒ/ê¸°ì ˆ'], ['Sent_666', 'ê·¸ëƒ¥ ê°ì •ì´ì…ì´ ì‹¬í•˜ê²Œ ë˜ê³  ë¶ˆì•ˆê°ë„ ì˜ ëŠë¼ëŠ” ê²ƒ ê°™ì•„ìš”.', 'ê°ì •/ë¶ˆì•ˆê°', 'ê°ì •/ë¶ˆì•ˆê°']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hALX4zcnxnfy",
        "colab_type": "text"
      },
      "source": [
        "#### Step 2. BERT ì¸í’‹ ë§Œë“¤ê¸° - í•¨ìˆ˜ ì •ì˜\n",
        "\n",
        "- í•™ìŠµì„ ìœ„í•´ ìì—°ì–´ ë¬¸ì¥ì— BPEë¥¼ ì ìš©í•˜ê³ , input_ids, segment_ids, input_masksë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "- ì´ í•¨ìˆ˜ì—ì„œëŠ” í•™ìŠµì„ ìœ„í•´ Ground Truth ë¼ë²¨ê¹Œì§€ ì •ë³´ë¥¼ InputFeaturesë¼ëŠ” í´ë˜ìŠ¤ë¥¼ ì´ìš©í•´ ì €ì¥í•˜ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLNHWwT91GgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5HbPHS3SS8-",
        "colab_type": "text"
      },
      "source": [
        "<font color = \"red\">[MISSION] convert_examples_to_features í•¨ìˆ˜ ì™„ì„±í•˜ê¸°</font>   \n",
        "\n",
        "convert_examples_to_featuresëŠ” \n",
        "- examples (ë°ì´í„°ì˜ ë¦¬ìŠ¤íŠ¸) \n",
        "- max_seq_length (ì¸í’‹ ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´)\n",
        "- tokenizer (ë¡œë”©í•œ BERTí† í¬ë‚˜ì´ì €)\n",
        "- label_map (ì¸ë±ìŠ¤ ë§¤í•‘ ì‚¬ì „)   \n",
        "\n",
        "ë¥¼ ì¸í’‹ìœ¼ë¡œ ë°›ì•„ InputFeaturesë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.   \n",
        "ì•„ë˜ ì½”ë“œì—ì„œ [â˜…CODE] ë¶€ë¶„ì„ ìˆ˜ì •í•˜ì—¬ í•¨ìˆ˜ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ë„ë¡ ìˆ˜ì •í•´ë³´ì„¸ìš”.\n",
        "\n",
        "<i>** ì •ë‹µì€ Wire í˜ì´ì§€ë¥¼ í†µí•´ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. </i>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRBcsKMWdeoF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_examples_to_features(examples, max_seq_length, tokenizer, label_map):\n",
        "    # features : ë¦¬í„´í•  InputFeaturesë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸\n",
        "    features = []\n",
        "\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        \"\"\"\n",
        "        í•¨ìˆ˜ì˜ ì¸í’‹ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ë¦¬ìŠ¤íŠ¸ëŠ” [ë¬¸ì¥ë²ˆí˜¸ - ë¬¸ì¥ - ë¼ë²¨]ì˜ ìˆœì„œë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
        "        ë¬¸ì¥ì„ text ë³€ìˆ˜ì— ì €ì¥í•˜ê³ , ë¼ë²¨ì„ label ë³€ìˆ˜ì— ì €ì¥í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
        "        labelì´ ì–´ë–¤ ì •ìˆ˜ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ”ì§€ëŠ” label_mapì„ í†µí•´ ì •ë³´ë¥¼ ë°›ì•„ label_id ë³€ìˆ˜ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
        "        \"\"\"\n",
        "        text = example[1] # ë¬¸ì¥\n",
        "        label = example[2] # ë¼ë²¨\n",
        "        label_id = label_map[label]\n",
        "\n",
        "        ## STEP 1. í˜•íƒœì†Œ ë¶„ì„ + BPEë¥¼ í†µí•´ í† í°í™”í•˜ê¸°\n",
        "        otk_tokenized_text = [â˜…CODE 1â˜…] # textì— í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ì ìš©\n",
        "        tokens_a = [â˜…CODE 2â˜…] # otk_tokenized_textì— BERT í† í¬ë‚˜ì´ì¦ˆ ì ìš©\n",
        "\n",
        "        ## ì£¼ì–´ì§„ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ë³´ë‹¤ ê¸´ ë¬¸ì¥ì€ ëë¶€ë¶„ì„ ì˜ë¼ëƒ„\n",
        "        ## [CLS], [SEP] í† í°ì´ ë°˜ë“œì‹œ í¬í•¨ë˜ë¯€ë¡œ max_seq_length - 2ë³´ë‹¤ ê¸´ ë¶€ë¶„ ìë¥´ê¸°\n",
        "        if len(tokens_a) > max_seq_length - 2:\n",
        "            tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "        ## STEP 2. BERT ì¸í’‹ í˜•ì‹ ë§ì¶”ê¸° : [CLS] ë¬¸ì¥ [SEP] í˜•íƒœ ë§Œë“¤ê¸°\n",
        "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
        "\n",
        "        ## STEP 3. segment_ids ë§Œë“¤ê¸° \n",
        "        ## -> ë‹¨ì¼ ë¬¸ì¥ì´ë¯€ë¡œ ëª¨ë‘ '0' ì¸ë±ìŠ¤ë¥¼ ë¶€ì—¬í•¨\n",
        "        segment_ids = [â˜…CODE 3â˜…]\n",
        "\n",
        "        ## STEP 4. í† í°ì„ input_idsë¡œ ë³€í™˜\n",
        "        ## hint: tokenizer.convert_tokens_to_ids ì‚¬ìš©\n",
        "        input_ids = [â˜…CODE 4â˜…]\n",
        "\n",
        "        ## STEP 5. ì¸í’‹ ë§ˆìŠ¤í¬ ë§Œë“¤ê¸° \n",
        "        ## -> ì‹¤ì œ í† í°ì´ ìˆëŠ” ë¶€ë¶„ì„ 1ë¡œ ì±„ì›€\n",
        "        input_mask = [â˜…CODE 5â˜…]\n",
        "\n",
        "        ## max_seq_lengthë³´ë‹¤ ëª¨ìë¥¸ ë¶€ë¶„ì„ paddingìœ¼ë¡œ ì±„ì›€ \n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        if ex_index < 3:\n",
        "            print(\"*** Example ***\")\n",
        "            print(\"tokens: %s\" % \" \".join(\n",
        "                    [str(x) for x in tokens]))\n",
        "\n",
        "            print(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            print(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "            print(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "            print(\"label: %d\" % (label_id))\n",
        "\n",
        "        features.append(\n",
        "                InputFeatures(input_ids=input_ids,\n",
        "                              input_mask=input_mask,\n",
        "                              segment_ids=segment_ids,\n",
        "                              label_id=label_id))\n",
        "    return features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhFAFdwxacX6",
        "colab_type": "text"
      },
      "source": [
        "#### Step 3. BERT ì¸í’‹ ë§Œë“¤ê¸° - ë°ì´í„°ì— í•¨ìˆ˜ ì ìš©\n",
        "- ì´ì œ ì •ì˜í•œ convert_examples_to_features í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ìš°ë¦¬ì˜ train ë°ì´í„°ë¥¼ BERTì¸í’‹ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.\n",
        "- ë¨¼ì € 176ê°œì˜ ë¼ë²¨ì„ 0~175ì˜ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë§¤í•‘í•˜ëŠ” label_mapì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "- ì´í›„ train ë°ì´í„°ì— í•¨ìˆ˜ë¥¼ ì ìš©í•´ train_featuresë¼ëŠ” ë³€ìˆ˜ì— ì €ì¥í•˜ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NBEQ4iBbkVv",
        "colab_type": "text"
      },
      "source": [
        "<font color = \"red\">[MISSION] ì „ì²˜ë¦¬ ê²°ê³¼ input_ids, input_mask, segment_idê°€ ì˜ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7FZaDBn9EoC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "47bf1b9a-c71b-4311-b07d-f3e926103162"
      },
      "source": [
        "# label mapping ì‚¬ì „ ë§Œë“¤ê¸°\n",
        "label_map = {}\n",
        "label_id = 0\n",
        "for dat in train:\n",
        "  if dat[2] not in label_map:\n",
        "    label_map[dat[2]] = label_id\n",
        "    label_id += 1\n",
        "\n",
        "## ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´ë¥¼ 128ìœ¼ë¡œ ì„¤ì •í•˜ê³  feature ë§Œë“¤ê¸°\n",
        "\n",
        "MAX_SEQ_LENGTH = 128\n",
        "print(\"*** Convert Train Examples ***\")\n",
        "train_features = convert_examples_to_features(train, MAX_SEQ_LENGTH, tokenizer, label_map)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** Convert Train Examples ***\n",
            "*** Example ***\n",
            "tokens: [CLS] ë­” ##ê°€ í•˜ ##ë£¨ ì¢… ##ì¼ ì´ ##ë ‡ê²Œ ë“¤ ##ëœ¬ ê¸° ##ë¶„ ì´ë‹¤ ë³´ ë‹ˆ ##ê¹Œ ì  ##ë„ ì˜ ì•ˆ ì™€ . [SEP]\n",
            "input_ids: 101 9304 11287 9952 35866 9684 18392 9638 82838 9117 118847 8932 37712 30919 9356 9049 118671 9655 12092 9654 9521 9590 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label: 0\n",
            "*** Example ***\n",
            "tokens: [CLS] ì•„ ##ë¬´ í•œ í…Œ ë‚˜ í™” ë‚´ ##ê³  ê·¸ ##ëŸ¬ ##ì§€ëŠ” ì•Š ##ì•„ . [SEP]\n",
            "input_ids: 101 9519 32537 9954 9866 8982 9993 8996 11664 8924 30873 32815 9523 16985 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label: 1\n",
            "*** Example ***\n",
            "tokens: [CLS] ì  ##ì ##ë¦¬ ì— ëˆ„ ##ì›Œ ##ë„ ë§¨ ##ë‚  ë’¤ ##ì²™ ì´ ##ê³  . . . ì  ì„ ì œ ##ëŒ€ë¡œ ì˜ ìˆ˜ ìˆì„ ë¦¬ ##ê°€ ì—† ##ì§€ . [SEP]\n",
            "input_ids: 101 9655 13764 12692 9559 9032 69592 12092 9260 41919 9109 119259 9638 11664 119 119 119 9655 9633 9672 37601 9654 9460 68943 9238 11287 9555 12508 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhDvTHhodlmx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "2cb8ee21-cd89-4a11-b988-b7f2bf118542"
      },
      "source": [
        "print(label_map)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'ì¦ìƒ/ë¶ˆë©´': 0, 'ê°ì •/ë¶„ë…¸': 1, 'ì¦ìƒ/ê¸°ì ˆ': 2, 'ê°ì •/ë¶ˆì•ˆê°': 3, 'ì¦ìƒ/ë‘ê·¼ê±°ë¦¼': 4, 'ë°°ê²½/ê±´ê°•ë¬¸ì œ': 5, 'ë°°ê²½/ì·¨ì—…': 6, 'ê°ì •/ìš°ìš¸ê°': 7, 'ë°°ê²½/ëŒ€ì¸ê´€ê³„': 8, 'ë°°ê²½/ì–´ë¦°ì‹œì ˆ': 9, 'ê°ì •/ìì‚´ì¶©ë™': 10, 'ê°ì •/ì™¸ë¡œì›€': 11, 'ì¦ìƒ/ìì‚´ì‹œë„': 12, 'ê°ì •/ê±±ì •': 13, 'ë°°ê²½/í•™ì—…': 14, 'ì¦ìƒ/ë¬´ê¸°ë ¥': 15, 'ì¦ìƒ/ì‹ìš•ì €í•˜': 16, 'ì¼ë°˜ëŒ€í™”': 17, 'ë°°ê²½/ê²°í˜¼': 18, 'ìƒíƒœ/ì¦ìƒì§€ì†': 19, 'ë°°ê²½/í•™êµ': 20, 'ê°ì •/ì§œì¦': 21, 'ë°°ê²½/ë¶€ëª¨': 22, 'ì¦ìƒ/ë‘í†µ': 23, 'ì¦ìƒ/í”¼í•´ë§ìƒ': 24, 'ê°ì •/ìì¡´ê°ì €í•˜': 25, 'ë°°ê²½/ì„±ê²©': 26, 'ì¦ìƒ/ê¸°ì–µìƒì‹¤': 27, 'ë°°ê²½/ì‚¬ì—…': 28, 'ê°ì •/ê°ì •ì¡°ì ˆì´ìƒ': 29, 'ë°°ê²½/ì „ì—°ì¸': 30, 'ê°ì •/ë¶ˆí¸ê°': 31, 'ë°°ê²½/ê²½ì œì ë¬¸ì œ': 32, 'ì¹˜ë£Œì´ë ¥/ë³‘ì›ë‚´ì›': 33, 'ë°°ê²½/ì¹œêµ¬': 34, 'ë°°ê²½/ê°€ì¡±': 35, 'ë°°ê²½/ì—¬ìì¹œêµ¬': 36, 'ì¹˜ë£Œì´ë ¥/ê²€ì‚¬': 37, 'ì¦ìƒ/ì–´ì§€ëŸ¬ì›€': 38, 'ê°ì •/ë¶€ì •ì ì‚¬ê³ ': 39, 'ê°ì •/ëˆˆë¬¼': 40, 'ë°°ê²½/ì‹œëŒ': 41, 'ì¦ìƒ/ë°˜ë³µí–‰ë™': 42, 'ê°ì •/ì¢Œì ˆ': 43, 'ë°°ê²½/ìë…€': 44, 'ê°ì •/ë¬´ë ¥ê°': 45, 'ë°°ê²½/ì‚¬ê³ ': 46, 'ë°°ê²½/ì§ì¥': 47, 'ì¦ìƒ/ê¸°ì–µë ¥ì €í•˜': 48, 'ì¦ìƒ/ì€ë‘”': 49, 'ì¦ìƒ/í™˜ì²­': 50, 'ì¦ìƒ/ì´ëª…': 51, 'ë¶€ê°€ì„¤ëª…': 52, 'ê°ì •/ì‹¬ë€': 53, 'ê°ì •/ë‹µë‹µ': 54, 'ê°ì •/í˜ë“¦': 55, 'ê°ì •/ê³µí—ˆê°': 56, 'ë°°ê²½/ë‚¨í¸': 57, 'ì¦ìƒ/ë°˜ë³µì‚¬ê³ ': 58, 'ê°ì •/ë¬´ì„œì›€': 59, 'ê°ì •/ë‘ë ¤ì›€': 60, 'ì¦ìƒ/í”¼ë¡œ': 61, 'ê°ì •/ì†ìƒí•¨': 62, 'ê°ì •/í†µì œë ¥ìƒì‹¤': 63, 'ë°°ê²½/ìƒí™œ': 64, 'ë°°ê²½/ì¢…êµ': 65, 'ê°ì •/ìê´´ê°': 66, 'ì¦ìƒ/í™˜ê°': 67, 'ë°°ê²½/ìŒì£¼': 68, 'ë°°ê²½/ëŒ€í•™': 69, 'ìê°€ì¹˜ë£Œ/ì‹¬ë¦¬ì¡°ì ˆ': 70, 'ê°ì •/ë‹¹í™©': 71, 'ê°ì •/ì¦ê±°ì›€': 72, 'ë°°ê²½/ë‚¨ìì¹œêµ¬': 73, 'ì¦ìƒ/ìƒë¦¬ë¶ˆìˆœ': 74, 'ê°ì •/ë¶ˆì¾Œê°': 75, 'ì¦ìƒ/ê³µí™©ë°œì‘': 76, 'ê°ì •/ì–µìš¸í•¨': 77, 'ê°ì •/ë°°ì‹ ê°': 78, 'ì¦ìƒ/í˜¸í¡ê³¤ë€': 79, 'ê°ì •/ì‹ ê²½ì“°ì„': 80, 'ë°°ê²½/ë¬¸ì œ': 81, 'ë°°ê²½/ê·€êµ­': 82, 'ë°°ê²½/ì• ì™„ë™ë¬¼': 83, 'ë°°ê²½/íƒ€ì¸': 84, 'ê°ì •/ì ˆë§ê°': 85, 'ì¦ìƒ/ì´ì¸ê°': 86, 'ì¦ìƒ/ëŒ€í™”ê¸°í”¼': 87, 'ê°ì •/ì„œìš´í•¨': 88, 'ë‚´ì›ì´ìœ /ì¹˜ë£Œ': 89, 'ê°ì •/ê´´ë¡œì›€': 90, 'ì¹˜ë£Œì´ë ¥/ì‘ê¸‰ì‹¤': 91, 'ê°ì •/ìì‹ ê°ì €í•˜': 92, 'ê°ì •/í™”': 93, 'ê°ì •/ì¶©ê²©': 94, 'ì¦ìƒ/ì§‘ì¤‘ë ¥ì €í•˜': 95, 'ì¦ìƒ/ìí•´': 96, 'ë°°ê²½/ì´ì‚¬': 97, 'ë°°ê²½/ì—°ì• ': 98, 'ê°ì •/ì‚´ì¸ìš•êµ¬': 99, 'ê°ì •/ë¶ˆì‹ ': 100, 'ê°ì •/ê¸°ë¶„ì €í•˜': 101, 'ê°ì •/ì˜ìš•ìƒì‹¤': 102, 'ë°°ê²½/êµ°ëŒ€': 103, 'ê°ì •/ê³µí¬': 104, 'ì¦ìƒ/ê³¼ëŒ€ë§ìƒ': 105, 'ë°°ê²½/ìê°': 106, 'ìƒíƒœ/ì–‘í˜¸': 107, 'ì¦ìƒ/í­ì‹': 108, 'ê°ì •/ì˜ˆë¯¼í•¨': 109, 'ê°ì •/ëª¨í˜¸í•¨': 110, 'ì¦ìƒ/ê³¼ìˆ˜ë©´': 111, 'ì¦ìƒ/ëŒ€ì¸ê¸°í”¼': 112, 'ì¦ìƒ/í˜ë¹ ì§': 113, 'ë°°ê²½/ì„ì‹ ': 114, 'ì¦ìƒ/ì²´ì¤‘ê°ì†Œ': 115, 'ë°°ê²½/ì´í˜¼': 116, 'ê°ì •/ê¸´ì¥': 117, 'ì¦ìƒ/ì£½ìŒê³µí¬': 118, 'ê°ì •/ì˜ê¸°ì†Œì¹¨': 119, 'í˜„ì¬ìƒíƒœ/ì¦ìƒì§€ì†': 120, 'ì¦ìƒ/ì²´ë ¥ì €í•˜': 121, 'ì¦ìƒ/í†µì¦': 122, 'ì¦ìƒ/ì•…ëª½': 123, 'ì¦ìƒ/ê³µê²©ì ì„±í–¥': 124, 'ì¦ìƒ/ì»¨ë””ì…˜ì €ì¡°': 125, 'ì¦ìƒ/ì„±ìš•ìƒìŠ¹': 126, 'ê°ì •/ê³ ë…ê°': 127, 'ì¦ìƒ/ê°€ìŠ´í†µì¦': 128, 'ë°°ê²½/ìœ¡ì•„': 129, 'ê°ì •/í›„íšŒ': 130, 'ë°°ê²½/ì•„ë¥´ë°”ì´íŠ¸': 131, 'ë‚´ì›ì´ìœ /ìƒë‹´': 132, 'ë°°ê²½/ê³µë¶€': 133, 'í˜„ì¬ìƒíƒœ/ì¦ìƒì•…í™”': 134, 'ì¦ìƒ/ì‹œë ¥ì €í•˜': 135, 'ì¦ìƒ/ì‹ ì²´ì´ìƒ': 136, 'ì¦ìƒ/ê±´ê°•ì—¼ë ¤': 137, 'ê°ì •/ë¬´ë¯¸ê±´ì¡°': 138, 'ê°ì •/ë¶ˆë§Œ': 139, 'ê°ì •/ë¯¸ì›€': 140, 'ì¦ìƒ/ì•Œì½”ì˜¬ì˜ì¡´': 141, 'ì¦ìƒ/ê¸°ì ˆì˜ˆê¸°': 142, 'ê°ì •/ìŠ¬í””': 143, 'ê°ì •/ìƒê°': 144, 'ì¦ìƒ/ë§Œì„±í”¼ë¡œ': 145, 'í˜„ì¬ìƒíƒœ/ì¦ìƒê°ì†Œ': 146, 'ê°ì •/ê³¼ë¯¼ë°˜ì‘': 147, 'ì¦ìƒ/ê°€ìŠ´ë–¨ë¦¼': 148, 'ê°ì •/ê¸°ì‹œê°': 149, 'ê°ì •/ë¯¸ì•ˆí•¨': 150, 'ê°ì •/í—ˆë¬´í•¨': 151, 'ì¦ìƒ/ì²´ì¤‘ì¦ê°€': 152, 'ê°ì •/ë©í•¨': 153, 'ê°ì •/ì£„ì±…ê°': 154, 'ì¦ìƒ/ë©”ìŠ¤êº¼ì›€': 155, 'ê°ì •/ë¹„ê´€ì ': 156, 'ìê°€ì¹˜ë£Œ/ìš´ë™': 157, 'ìê°€ì¹˜ë£Œ/ì¶©ë¶„í•œíœ´ì‹': 158, 'ì¦ìƒ/ë°œì‘': 159, 'ì¦ìƒ/ì¸ì§€ê¸°ëŠ¥ì €í•˜': 160, 'ë‚´ì›ì´ìœ /ì˜ì‚¬ì†Œê²¬': 161, 'ì¦ìƒ/ì €ë¦¼í˜„ìƒ': 162, 'ì¦ìƒ/ê°€ìŠ´ë‹µë‹µ': 163, 'ì¦ìƒ/ì„±ê²©ë³€í™”': 164, 'ì¦ìƒ/ì†Œí™”ë¶ˆëŸ‰': 165, 'ì¦ìƒ/í¸ë‘í†µ': 166, 'ì¦ìƒ/ë–¨ë¦¼': 167, 'ë°°ê²½/ìœ í•™': 168, 'ìƒíƒœ/ì¦ìƒê°ì†Œ': 169, 'ê°ì •/ì°½í”¼í•¨': 170, 'ì¦ìƒ/ì†ì“°ë¦¼': 171, 'ê°ì •/ê³¤í˜¹ê°': 172, 'ì›ì¸/ì—†ìŒ': 173, 'ê°ì •/ì´ˆì¡°í•¨': 174, 'ë°°ê²½/ì§„ë¡œ': 175}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONz8hu3vb7II",
        "colab_type": "text"
      },
      "source": [
        "#### Step 4. ì „ì²˜ë¦¬ ê²°ê³¼ í™•ì¸ & ë“œë¼ì´ë¸Œì— ì €ì¥\n",
        "- BERT í† í¬ë‚˜ì´ì§• ê²°ê³¼, [UNK]ê°€ ì–¼ë§ˆë‚˜ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "- í˜„ì¬ ë°ì´í„°ì˜ ê²½ìš°, í•™ìŠµ ë°ì´í„° ì¤‘ 1.34%ê°€ [UNK] í† í°ìœ¼ë¡œ ë–¨ì–´ì§„ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "- êµ¬ê¸€ì˜ ëª¨ë¸ì€ 102êµ­ì–´ì— ëŒ€í•´ í•™ìŠµëœ ëª¨ë¸ìœ¼ë¡œ, í•œêµ­ì–´ë§Œìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸ì— ë¹„í•´ UNK ë¹„ìœ¨ì´ ë†’ì€ í¸ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEEvpbzz8R2W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4cce563e-c57b-4070-a163-798a54982929"
      },
      "source": [
        "tot_tokens = 0\n",
        "num_oov = 0\n",
        "for feat in train_features:\n",
        "  num_oov += sum([s == 100 for s in feat.input_ids])\n",
        "  tot_tokens += sum([s != 0 for s in feat.input_ids])\n",
        "\n",
        "print(\"í•™ìŠµ ë°ì´í„° ì¤‘ {}ê°œ ({:.2f}%) í† í°ì€ [UNK] ì²˜ë¦¬ë¨\".format(num_oov, 100*num_oov/tot_tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "í•™ìŠµ ë°ì´í„° ì¤‘ 1351ê°œ (1.34%) í† í°ì€ [UNK] ì²˜ë¦¬ë¨\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzsJXe4ncnqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open(\"/content/gdrive/My Drive/NLP/BERT_CLS_train_features.pkl\", \"wb\") as f:\n",
        "  pickle.dump(train_features, f)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}